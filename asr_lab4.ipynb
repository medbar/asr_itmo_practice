{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceaa7b0d-bc13-4085-90cf-513b0e6c8ddc",
   "metadata": {},
   "source": [
    "# Практическая работа по распознаванию речи #4: <br> Поиск наилучшей гипотезы распознавания\n",
    "Задача распознавания речи состоит в поиске наиболее вероятной словной гипотезы по имеющемуся звуковому сигналу. Используя теорему Байеса, формулировку задачи можно написать так:\n",
    "$$ W^* = \\underset{W}{argmax} {P(W|O)} =  \\underset{W}{argmax} \\sum_i^N{(\\log{P(O|w_i)} + \\log{P(w_i|w_{i-1}, w_{i-2},..))}}$$\n",
    "Где: \n",
    "* $O$ - звук\n",
    "* $W$ - словная гипотеза распознавания \n",
    "* $W^*$ - лучшая гипотеза распознавания\n",
    "* $P(W|O)$ - вероятность гипотезы распознавания при условии наблюдения $O$\n",
    "* $N$ - количество слов в гипотезе\n",
    "* $w_i$ - i'ое слово в гипотезе\n",
    "* $P(O|w_i)$ - акустическое правдоподобие слова (выводится из предсказания акустической моделью)\n",
    "* $P(w_i|w_{i-1}, w_{i-2},..)$ - языковая вероятность слова при условии контекста (предсказывается языковой моделью)\n",
    "\n",
    "\n",
    "В прошлых лабораторных работах были изучены акустическая и языковая модели. Акустическая модель предсказывает вероятность принадлежности кадра некоему акустическому классу (фонеме). Языковая модель предсказывает априорную вероятность последовательности слов. Пришло время разобраться, как соединить эти части в одну систему и получить итоговый результат распознавания. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Основная часть (14 баллов) данной практической работы состоит из двух частей: \n",
    "* знакомство с Weighted Finite State Transducer (WFST)\n",
    "* WFST декодинг в ASR системе\n",
    "\n",
    "Дополнительная часть - тюнинг параметров (2 балла)\n",
    "\n",
    "## Полезные ссылки: \n",
    "* Наиболее популярная библиотека WFST - [OpenFst](https://www.openfst.org/twiki/bin/view/FST/WebHome)\n",
    "* Библиотека для визуализации - [graphviz](https://graphviz.readthedocs.io/en/stable/manual.html)\n",
    "* Алгоритм обхода графа в ширину - [BFS](https://neerc.ifmo.ru/wiki/index.php?title=%D0%9E%D0%B1%D1%85%D0%BE%D0%B4_%D0%B2_%D1%88%D0%B8%D1%80%D0%B8%D0%BD%D1%83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e04b33-5e49-4ecf-8608-6710eb6c24dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "!python3 --version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b036c8bf-6f0e-4d80-9d43-016b041c9216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get install cmake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5fc2f9b-c91b-44d1-b8b2-114a8836ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install graphviz kenlm kaldiio jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "747cd3cb-0d09-4ecb-8468-7341fc70b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import abc\n",
    "\n",
    "from typing import List, Dict, Union, Set, Any, Optional, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "import graphviz\n",
    "import kenlm \n",
    "import jiwer\n",
    "from kaldiio import ReadHelper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dc3ee2-63c9-4fdb-a7ab-3fbace5043d4",
   "metadata": {},
   "source": [
    "# 1. Weighted Finite State Transducer (WFST)\n",
    "\n",
    "\n",
    "\n",
    "Из определения в wiki [Weighted Finite State Transducer (Взвешенный конечный автомат с выходом)](https://en.wikipedia.org/wiki/Finite-state_transducer) следует, что главная задача FST, --- это переводить символы из входного алфавита в соответствующие им символы из выходного алфавита. Например, с помощью FST можно перевести последовательность фонем в последовательность слов. Конечный автомат будет менять свое состояние при обработке последовательности входных символов (фонем). Когда он соберет из входных фонем корректную транскрипцию слова, то выдаст это слово на выход. \n",
    "\n",
    "Однако не всегда можно однозначно перевести последовательность фонем в слова (некоторые слова произносятся одинаково, а пишутся по-разному), поэтому для распознавания речи надо использовать не простой FST, а взвешенный. WFST позволяет не только перевести входные символы в выходные, но и оценить вес такого перевода. Чем больше вес, тем хуже гипотеза.  \n",
    "\n",
    "Напишем собственную реализацию WFST.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f84131-244f-4672-bb95-f245385b6a88",
   "metadata": {},
   "source": [
    "### AbstractWFST\n",
    "AbstractWFST -- это базовый интерфейс нашего WFST. В нем заданы основные методы, через которые мы будем работать с wfst  \n",
    "* get_start - возвращаяет стартовое состояние конечного автомата.\n",
    "* final_score - вес завершения работы в данном состоянии. В некоторых состояниях завершить обработку невозможно, в таких случаях final weight  будет равен бесконечности.\n",
    "*  transduce - совершает переход из текущего состояния в следующее, соответствующее переходу по символу ilabel. Возвращает выходной символ, вес перехода и следующее состояние.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86425820-9a05-4463-8c97-5d749dc43f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractWFST(abc.ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def get_start(self) -> Any:\n",
    "        \"\"\"Return start state\"\"\"\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def final_score(self, state: Any) -> float:\n",
    "        \"\"\"is the state final? If true return negative log likelihood of the finalization. otherwise return inf\"\"\"\n",
    "           \n",
    "    @abc.abstractmethod\n",
    "    def transduce(self, state: Any, ilabel: str) -> Tuple[Tuple[str, float, Any]]:\n",
    "        \"\"\"Transduce ilabel to olabel. \n",
    "        return all available olabels for this state and ilabel pair\n",
    "        return type - ((olabel1, weight1, nextstate1), \n",
    "                       ((olabel1, weight1, nextstate1), \n",
    "                       ...)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df77d5-b466-4eb4-8179-51ed23b7b0db",
   "metadata": {},
   "source": [
    "### SymbolsMap\n",
    "\n",
    "Для того, чтобы задать WFST, первым делом нам понадобятся таблицы входных и выходных символов. Для задания таблиц будем использовать класс SymbolsMap. Данный класс служит для маппинга символов на индексы и обратно. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4441149-62c8-4255-b240-4652f8ff9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymbolsMap:\n",
    "    def __init__(self, id2symbol: Optional[Dict[int, str]] = None, symbol2id: Optional[Dict[str, int]] = None):\n",
    "        assert id2symbol is not None or symbol2id is not None, f\"One id2symbol or symbol2id must be not None\"\n",
    "        assert id2symbol is None or symbol2id is None, f\"Only one One id2symbol or symbol2id can be not None\"\n",
    "        if id2symbol is None:\n",
    "            self.id2symbol = {i:s for s,i in symbol2id.items()}\n",
    "        elif isinstance(id2symbol, dict):\n",
    "            self.id2symbol = id2symbol\n",
    "        elif isinstance(id2symbol, list):\n",
    "            self.id2symbol = {i:s for i, s in enumerate(id2symbol)}\n",
    "        else:\n",
    "            raise RuntimeError(f\"unknown type {type(id2symbol)=}\")\n",
    "            \n",
    "        if symbol2id is None:\n",
    "            self.symbol2id = {s:i for i, s in self.id2symbol.items()}\n",
    "        else:\n",
    "            self.symbol2id = symbol2id   \n",
    "                \n",
    "        assert self.id2symbol[0] == '<eps>', f\"wrong {self.id2symbol}\"\n",
    "\n",
    "    def get_id(self, symbol: str):\n",
    "        return self.symbol2id[symbol]\n",
    "        \n",
    "    def get_symbol(self, id: int):\n",
    "        return self.id2symbol[id] \n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, fname):\n",
    "        \"\"\"read symbols table from file\n",
    "        format: \n",
    "            word id\n",
    "            word2 id2\n",
    "            ...\n",
    "        \"\"\"\n",
    "        with open(fname) as f:\n",
    "            s2i = {s:int(i) for s, i in map(str.split, f.readlines())}\n",
    "        return cls(symbol2id=s2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb6a626-366b-4ff9-ba63-0c2f03818367",
   "metadata": {},
   "source": [
    "### Arc и ILabelIndexedArcs\n",
    "Переходы внутри FST будем описывать с помощью класса, инкапсулирующего информацию о входном/выходном индексе символа, весе перехода и следующем состоянии FST. Поскольку дуги мы будем хранить отдельно для каждого состояния, информация о текущем состоянии в дуге не нужна. \n",
    "\n",
    "Для быстрого выбора нужной дуги создадим специальную коллекцию ILabelIndexedArcs. Данная коллекция хранит дуги таким образом, чтобы выбор дуги по входному символу осуществлялся за O(1). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a098c7d-1d87-4e65-81c8-43a18e4df4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Arc:\n",
    "    \"\"\"Arc in WFST\"\"\"\n",
    "    ilabel: int\n",
    "    olabel: int\n",
    "    weight: float\n",
    "    nextstate: Any\n",
    "\n",
    "class ILabelIndexedArcs:\n",
    "    def __init__(self, arcs: Optional[List[Arc]] = None):\n",
    "        self.ilabel2arclist = defaultdict(list)\n",
    "        if arcs is not None:\n",
    "            for arc in arcs:\n",
    "                self.add_arc(arc)\n",
    "\n",
    "    def add_arc(self, arc: Arc):\n",
    "        self.ilabel2arclist[arc.ilabel].append(arc)\n",
    "\n",
    "    def get_arcs_by_ilabel(self, ilabel: int):\n",
    "        return self.ilabel2arclist[ilabel]\n",
    "\n",
    "    def arcs(self) -> List[Arc]:\n",
    "        \"\"\"Return all arcs\"\"\"\n",
    "        all_arcs = []\n",
    "        # TODO\n",
    "        # реализуйте функцию, которая возвращает все дуги, хранящиеся в данной коллекции\n",
    "        #raise NotImplementedError()\n",
    "        for arclist in self.ilabel2arclist.values():\n",
    "            all_arcs.extend(arclist)\n",
    "        # !!!!!!!!!!!!\n",
    "        return all_arcs \n",
    "\n",
    "    def __str__(self):\n",
    "        return \"ILabelIndexedArcs([\" + \", \".join(f\"{a}\" for a in self.arcs()) + \"])\"\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0bc0d83-fecf-42e6-9dd1-4e95622cc6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1.a passed\n"
     ]
    }
   ],
   "source": [
    "# test 1.a\n",
    "def test_ILabelIndexedArcs():\n",
    "    # проверка что arcs работает правильно \n",
    "    arcs = [Arc(i, -i, i/100, i+100) for i in range(10)]\n",
    "    c = ILabelIndexedArcs(arcs)\n",
    "    arcs2 = c.arcs()\n",
    "    \n",
    "    assert len(arcs) == len(arcs2), f\"{len(arcs)=}, {len(arcs2)=}\\n{arcs} != {c.arcs()}\"\n",
    "    for a in arcs2:\n",
    "        assert a in arcs, f\"{a=} not in {arcs=}\"\n",
    "    print('Test 1.a passed')\n",
    "test_ILabelIndexedArcs()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d7679-8e0c-41d8-8ac5-bdd38841594f",
   "metadata": {},
   "source": [
    "### WFST \n",
    "Все готово для создания взвешенного конечного автомата. Класс WFST состоит из таблицы входных и выходных символов, списка состояний и коллекции дуг для каждого, а также множества финальных состояний. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb8dab12-e963-44c8-98a1-cde9f8044c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WFST(AbstractWFST):\n",
    "    def __init__(self, \n",
    "                 isymbols: Optional[SymbolsMap] = None, \n",
    "                 osymbols: Optional[SymbolsMap] = None):\n",
    "        self.start = 0\n",
    "        self.states = [0]\n",
    "        # состояния, в которых может завершиться декодирование без какого либо штрафа \n",
    "        self.final_states = set() \n",
    "        self.isymbols = isymbols\n",
    "        self.osymbols = osymbols\n",
    "        self.state2arcs = defaultdict(ILabelIndexedArcs)\n",
    "\n",
    "    def get_start(self):\n",
    "        \"\"\"return start state\"\"\"\n",
    "        return self.start\n",
    "        \n",
    "    def final_score(self, state_id: int):\n",
    "        \"\"\"return the weight of decoding completion in state_id\"\"\"\n",
    "        # TODO \n",
    "        # верните вес завершения декодирования в state_id \n",
    "        #raise NotImplementedError()\n",
    "        # !!!!!!!!!!!!!!!!!!!\n",
    "        if state_id in self.final_states:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return float('inf')\n",
    "       \n",
    "    def transduce(self, state, ilabel: str) -> Tuple[Tuple[str, float, int]]:\n",
    "        \"\"\"transitions wfst to next state by 'ilabel' input symbol, except when the input symbol is <eps>.\n",
    "        Returns all possible output results as a tuple of triples (olabel, weight, nextstate).\"\"\"\n",
    "        assert self.isymbols is not None and self.osymbols is not None, f\"Cannot transduce w/o both symbols tables\" \n",
    "        if ilabel == '<eps>':\n",
    "            # skip <eps> input\n",
    "            return ((ilabel, 0, state), )\n",
    "        label_id = self.isymbols.get_id(ilabel)\n",
    "        # TODO \n",
    "        # верните все возможные результаты перехода из текущего состояния по входу ilabel\n",
    "        # результат должен выглядеть так: tuple((\"слово\", 0.1, 2), (\"другоеслово\", 10, 4), ...)\n",
    "        #raise NotImplementedError()\n",
    "        # !!!!!!!!!!!!\n",
    "        if ilabel == '<eps>':\n",
    "            return ((ilabel, 0, state), )\n",
    "        \n",
    "        label_id = self.isymbols.get_id(ilabel)\n",
    "        arcs = self.state2arcs[state].get_arcs_by_ilabel(label_id)\n",
    "        return tuple((self.osymbols.get_symbol(arc.olabel), arc.weight, arc.nextstate) for arc in arcs)\n",
    "\n",
    "    \n",
    "    def set_final(self, state: int):\n",
    "        \"\"\"sets the final weight for the state to zero\"\"\"\n",
    "        # TODO добавьте возможность завершать декодирование в state\n",
    "        #raise NotImplementedError()\n",
    "        # !!!!!!!!!!!\n",
    "        self.final_states.add(state)\n",
    "\n",
    "        \n",
    "    def new_state(self):\n",
    "        \"\"\"Create new state id and return it\"\"\"\n",
    "        # TODO \n",
    "        # добавьте в конечный автомат новое состояние и верните его id \n",
    "        #raise NotImplementedError()\n",
    "        # !!!!!!!!\n",
    "        state_id = max(self.states) + 1\n",
    "        self.states.append(state_id)\n",
    "        return state_id\n",
    "    \n",
    "    def add_arc(self, state_from: int, arc: Arc):\n",
    "        \"\"\"adds a new arc for this state\"\"\"\n",
    "        # TODO \n",
    "        # добавьте в конечный автомат новых переход arc, выходящий из state_from \n",
    "        #raise NotImplementedError()\n",
    "        # !!!!!!!!!!!!!!!!!!\n",
    "        self.state2arcs[state_from].add_arc(arc)\n",
    "    \n",
    "    def to_dot(self):\n",
    "        \"\"\"Visualize the WFST\"\"\" \n",
    "        dot = graphviz.Digraph()\n",
    "        for s in self.states:\n",
    "            dot.node(str(s))\n",
    "        for state_from, arcs_container in self.state2arcs.items():\n",
    "            for arc in arcs_container.arcs():\n",
    "                il = self.isymbols.get_symbol(arc.ilabel) if self.isymbols is not None else arc.ilabel\n",
    "                ol = self.osymbols.get_symbol(arc.olabel) if self.osymbols is not None else arc.olabel\n",
    "                dot.edge(str(state_from), str(arc.nextstate), label=f\"{il}:{ol}:{arc.weight:.2f}\")\n",
    "        return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8959ca8-4e58-4ea2-b4c9-d4ce12d800cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 1.b passed\n"
     ]
    }
   ],
   "source": [
    "# test 1.b\n",
    "def test_WFST():\n",
    "    en_l = SymbolsMap(id2symbol=['<eps>', 'b', 'd', 'u'])\n",
    "    ru_l = SymbolsMap(id2symbol=['<eps>', 'ю', 'б', 'д'])\n",
    "    \n",
    "    # create first transducer\n",
    "    en2ru = WFST(isymbols=en_l, osymbols=ru_l)\n",
    "    # new_state\n",
    "    en2ru_final = en2ru.new_state()\n",
    "    assert en2ru_final == 1 , f'start - zero, next state - one, next - two ...'\n",
    "    assert en2ru.new_state() == 2, f'start - zero, next state - one, next - two ...'\n",
    "    en2ru.set_final(en2ru_final)\n",
    "    en2ru.add_arc(en2ru.get_start(), Arc(1, 2, 1.0, en2ru_final))\n",
    "    en2ru.add_arc(en2ru_final, Arc(2, 3, 2.0, en2ru.get_start()))\n",
    "    en2ru.add_arc(en2ru.get_start(), Arc(3, 1, 3.0, en2ru_final))\n",
    "    # you can use this line to visualize\n",
    "    #display.display(en2ru.to_dot())\n",
    "\n",
    "    # final_score # set_final\n",
    "    assert en2ru.final_score(en2ru.get_start()) == float('inf')\n",
    "    assert en2ru.final_score(en2ru_final) == 0\n",
    "    en2ru.set_final(en2ru.get_start())\n",
    "    assert en2ru.final_score(en2ru_final) == en2ru.final_score(en2ru.get_start()) == 0\n",
    "\n",
    "        \n",
    "    # transduce # add_arc\n",
    "    assert en2ru.transduce(en2ru.get_start(), 'd') == tuple() , \"Cannot transduce 'd' from start state. output must be zero len tuple\"\n",
    "    assert en2ru.transduce(en2ru.get_start(), 'b') == (('б', 1.0, en2ru_final), ) , \"Arc(1, 2, 1.0, en2ru_final)\"\n",
    "    assert en2ru.transduce(en2ru.get_start(), 'u') == (('ю', 3.0, en2ru_final), ) , \"Arc(3, 1, 3.0, en2ru_final)\"\n",
    "    assert en2ru.transduce(en2ru_final, 'd') == (('д', 2.0, en2ru.get_start()), ) , \"Arc(2, 3, 2.0, en2ru.get_start())\"\n",
    "    \n",
    "    print('test 1.b passed')\n",
    "test_WFST()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b598f04d-11fe-4b37-a396-1eea00d351bd",
   "metadata": {},
   "source": [
    "## wfst композиция \n",
    "\n",
    "\n",
    "Над WFST определен большой набор различных операций, реализация большинства из которых является сложной алгоритмической задачей. Важной для распознавания речи операцией является [композиция](https://www.openfst.org/twiki/bin/view/FST/ComposeDoc) набора различных wfst. \n",
    "\n",
    "OpenFST дает следующее определение композиции:\n",
    " \n",
    "\"This operation computes the composition of two transducers. If A transduces string x to y with weight a and B transduces y to z with weight b, then their composition transduces string x to z with weight a ⊗ b.\"\n",
    "\n",
    "Другими словами, результатом композиции является wfst, применение которого к входной последовательности даст такой же результат, как и последовательное применение композируемых wfst. \n",
    "\n",
    "Хорошим примером композиции различных WFST является граф распознавания в гибридных системах распознавания речи. Такой граф является результатом композиции четырех wfst:\n",
    "* H - Hidden Markov Model wfst (переводит акустические классы, предсказанные с помощью AM, в трифоны)\n",
    "* С - Context-dependency transducer (переводит трифоны (тройки (leftcontext,phone,rightcontext)) в фонемы)\n",
    "* L - lexicon (переводит фонемы в слова)\n",
    "* G - ngram lm (оценивает вероятность последовательностей слов)\n",
    " \n",
    "Итого граф распознавания можно выразить с помощью следующей формулы:\n",
    "$$ HCLG = H⊗C⊗L⊗G $$ \n",
    "где ⊗ - оператор композиции двух wfst. Подробнее про граф распознавания можно почитать в [документации к фреймворку kaldi](https://kaldi-asr.org/doc/graph.html)\n",
    "\n",
    "\n",
    "Композиция позволяет объединить много обработчиков в один большой граф, что несомненно является большим плюсом для построения продакшн решений. Но у такого подхода есть и минусы - процесс подготовки графа очень сложен и требует множества оптимизаций. Это усложняет любые эксперименты и модификации системы. В данной работе мы не будем реализовывать честную композицию графа, а будем считать композицию \"On the Fly\". То есть сделаем обертку, реализующую интерфейс AbstractWFST, состоянием которой будет Tuple состояний всех композируемых wfst, а метод transduce будет последовательно проходить через эти wfst. Тем самым, по определению композиции, наш класс будет эквивалентен честной композиции.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a52b7dc4-b98d-468a-9d61-990eda9217d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnTheFlyCompose(AbstractWFST):\n",
    "    def __init__(self, wfsts: List[AbstractWFST]):\n",
    "        # !!!Attention!!! wfsts - это лист AbstractWFST, а не WFST. \n",
    "        # Для работы с wfsts истользуйте только методы из AbstractWFST\n",
    "        self.wfsts = wfsts\n",
    "\n",
    "    def get_start(self):\n",
    "        return tuple(fst.get_start() for fst in self.wfsts)\n",
    "\n",
    "    def final_score(self, state: Tuple[Any]):\n",
    "        assert len(state) == len(self.wfsts)\n",
    "        # TODO \n",
    "        # посчитайте финальный скор для state\n",
    "        # результат композиции может завершить обработку только в тех стейтах, где все wfsts будут в финальных состояниях\n",
    "        #raise NotImplementedError()\n",
    "        # !!!!!!!!!!!!!!!!!!!!!\n",
    "        final_weights = [fst.final_score(s) for fst, s in zip(self.wfsts, state)]\n",
    "        if all(w != float('inf') for w in final_weights):\n",
    "            return sum(final_weights)\n",
    "        else:\n",
    "            return float('inf')\n",
    "    \n",
    "    def _transduce(self, state: Tuple[Any], ilabel: str) -> List[Tuple[List[str], float, List[int]]]:\n",
    "        assert len(self.wfsts) == len(state)\n",
    "        # Найдите все возможные гипотезы перевода ilabel с помощью композиции всех self.wfsts\n",
    "        # Последовательно пройдите через все self.wfsts, расширяя список гипотез и дополняя гипотезы новыми слоями \n",
    "        # Верните результат в виде списка гипотез. \n",
    "        # Каждая гипотеза содержит три элемента: \n",
    "        #   0. список символов со ВСЕХ прошедших слоев композиции \n",
    "        #   1. суммарный вес гипотезы \n",
    "        #   2. список состояний, в которые перешли self.wfsts \n",
    "\n",
    "        # Одна стартовая гипотеза. Корень для всех гипотез.\n",
    "        # Еще никакой wfst не применен, символ только ilabel, вес стартовый, стейтов еще нет. \n",
    "        hyps_per_layer = [([ilabel], 0, [])]  \n",
    "        for fst, s in zip(self.wfsts, state):\n",
    "            new_hyps = []\n",
    "            # TODO \n",
    "            # примените fst.transduce ко всем гипотезам из hyps_per_layer\n",
    "            # постепенно разветвляя их и сохраняя в new_hyp\n",
    "            # на вход подайте выход последнего из уже обработаных слоев \n",
    "            #raise NotImplementedError()\n",
    "            for hyp in hyps_per_layer:\n",
    "                last_output, total_weight, states = hyp\n",
    "                for olabel, weight, nextstate in fst.transduce(s, last_output[-1]):\n",
    "                    # Обновляем выходную последовательность, вес и состояния\n",
    "                    new_output = last_output + [olabel]\n",
    "                    new_weight = total_weight + weight\n",
    "                    new_states = states + [nextstate]\n",
    "                    new_hyps.append((new_output, new_weight, new_states))\n",
    "            # !!!!!!!!!\n",
    "            hyps_per_layer = new_hyps\n",
    "            \n",
    "        return hyps_per_layer\n",
    "\n",
    "    def transduce(self, state: Tuple[Any], ilabel: str):\n",
    "        hyps_per_layer = self._transduce(state, ilabel)\n",
    "        # выходной символ transduce в композиции - это выходной символ самого последнего wfst \n",
    "        return tuple((ls[-1], w, tuple(ss)) for ls, w, ss in hyps_per_layer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de383686-63f9-4664-bceb-e6eed2f1c1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1.c passed\n"
     ]
    }
   ],
   "source": [
    "def test_OnTheFlyCompose():\n",
    "    en_l = SymbolsMap(id2symbol=['<eps>', 'b', 'd', 'u'])\n",
    "    ru_l = SymbolsMap(id2symbol=['<eps>', 'ю', 'д', 'б'])\n",
    "    en_U = SymbolsMap(id2symbol=['<eps>', 'B', 'D', 'U'])\n",
    "    ru_U = SymbolsMap(id2symbol=['<eps>', 'Ю', 'Д', 'Б'])\n",
    "    \n",
    "    # create first transducer\n",
    "    en2ru = WFST(isymbols=en_l, osymbols=ru_l)\n",
    "    en2ru_final = en2ru.new_state()\n",
    "    en2ru.set_final(en2ru_final)\n",
    "    en2ru.add_arc(en2ru.get_start(), Arc(1, 3, 1.0, en2ru_final))\n",
    "    en2ru.add_arc(en2ru_final, Arc(2, 2, 2.0, en2ru.get_start()))\n",
    "    en2ru.add_arc(en2ru.get_start(), Arc(2, 0, 0.1, en2ru.get_start()))\n",
    "    en2ru.add_arc(en2ru.get_start(), Arc(3, 1, 3.0, en2ru_final))\n",
    "    # you can use this line to visualize\n",
    "    # display.display(en2ru.to_dot())\n",
    "\n",
    "    # OnTheFlyCompose одного fst работает так же как и этот fst\n",
    "    comp = OnTheFlyCompose([en2ru])\n",
    "    for s in (0, 1):\n",
    "        for il in 'bdu':\n",
    "            hyps1 = en2ru.transduce(s, il)\n",
    "            hyps2 = comp.transduce((s,), il)\n",
    "            for h in hyps2:\n",
    "                # only one state\n",
    "                assert len(h[2]) == 1, f\"{hyps2=}\"\n",
    "            hyps2_flatten = tuple((l, w, ss[0]) for l,w,ss in hyps2)\n",
    "            assert hyps1 == hyps2_flatten, f\"{hyps1=} {hyps2_flatten=}\"\n",
    "\n",
    "    # create second transducer\n",
    "    ru2en = WFST(isymbols=ru_l, osymbols=en_U)\n",
    "    ru2en_final = ru2en.new_state()\n",
    "    ru2en.set_final(ru2en_final)\n",
    "    ru2en.add_arc(ru2en.get_start(), Arc(3, 1, 10.0, ru2en_final))\n",
    "    ru2en.add_arc(ru2en_final, Arc(2, 2, 20.0, ru2en_final))\n",
    "    ru2en.add_arc(ru2en_final, Arc(2, 2, 30.0, ru2en.get_start()))\n",
    "    ru2en.add_arc(ru2en_final, Arc(1, 3, 40.0, ru2en.get_start()))\n",
    "    \n",
    "    # you can use this line to visualize\n",
    "    # display.display(ru2en.to_dot())\n",
    "\n",
    "    # композиция двух работает как последовательное применение каждого \n",
    "    comp = OnTheFlyCompose([en2ru, ru2en])\n",
    "    hyps = comp.transduce((0, 0), 'b')\n",
    "    assert hyps == (('B', 11.0, (1, 1)), ) , f\"{hyps =}\"\n",
    "    hyps = comp.transduce((1, 1), 'd')\n",
    "    assert hyps == (('D', 22.0, (0, 1)), ('D', 32.0, (0, 0))) , f\"{hyps =}\"\n",
    "    hyps = comp.transduce((0, 1), 'd')\n",
    "    assert hyps == (('<eps>', 0.1, (0, 1)), ) , f\"{hyps =}\"\n",
    "    print('Test 1.c passed')\n",
    "    \n",
    "test_OnTheFlyCompose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615649a8-ea1b-4d59-be37-1a18fb509619",
   "metadata": {},
   "source": [
    "# 2. WFST декодинг в ASR системе\n",
    "Отлично, код WFST готов, теперь необходимо собрать граф распознавания и написать поиск наилучшей гипотезы \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cba40b2-2a44-47d2-897f-f26df9dfb51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сначала подготовим нужные нам таблицы символов\n",
    "AM_PHONES = {0: 'pau', 1: 'aa', 2: 'ae', 3: 'ah', 4: 'ao', 5: 'aw', 6: 'ax', 7: 'ax-h', 8: 'axr', 9: 'ay', 10: 'b', \n",
    "             11: 'bcl', 12: 'ch', 13: 'd', 14: 'dcl', 15: 'dh', 16: 'dx', 17: 'eh', 18: 'el', 19: 'em', 20: 'en', \n",
    "             21: 'eng', 22: 'er', 23: 'ey', 24: 'f', 25: 'g', 26: 'gcl', 27: 'hh', 28: 'hv', 29: 'ih', 30: 'ix', \n",
    "             31: 'iy', 32: 'jh', 33: 'k', 34: 'kcl', 35: 'l', 36: 'm', 37: 'n', 38: 'ng', 39: 'nx', 40: 'ow', \n",
    "             41: 'oy', 42: 'p', 43: 'pcl', 44: 'q', 45: 'r', 46: 's', 47: 'sh', 48: 't', 49: 'tcl', 50: 'th', \n",
    "             51: 'uh', 52: 'uw', 53: 'ux', 54: 'v', 55: 'w', 56: 'y', 57: 'z', 58: 'zh'}\n",
    "\n",
    "# таблица фонем для wfst \n",
    "def create_phones_txt(fname='exp/phones.txt'):\n",
    "    fname = Path(fname) \n",
    "    fname.parent.mkdir(exist_ok=True, parents=True)\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write('<eps> 0\\n') # !! shift all ids by one !! \n",
    "        for i, p in sorted(AM_PHONES.items()):\n",
    "            f.write(f'{p} {i+1}\\n')\n",
    "create_phones_txt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "565bc3b5-d319-481d-bf69-01251746fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим подготовленный лексикон из датасета \n",
    "def load_lexicon_file(fname='timit/TIMITDIC.TXT', words_limit=-1):\n",
    "    \"\"\"generator running through the 'fname' phonetic dictionary \n",
    "    yield (word: str, phones: List[str])\"\"\"\n",
    "    num = 0\n",
    "    with open(fname) as f:\n",
    "        for line in map(str.strip, f.readlines()):\n",
    "            if line.startswith(';'):\n",
    "                continue\n",
    "            word, trans, _ = line.split('/')\n",
    "            # remove ~adj suffix \n",
    "            word = word.split('~')[0].strip() \n",
    "            # remove stress factor\n",
    "            trans = [t[:-1] if t[-1].isdigit() else t for t in trans.split()]\n",
    "            yield word, trans\n",
    "            num += 1\n",
    "            if words_limit == num:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc001f37-c1b9-499b-8128-127892fda6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# подготовим таблицу слов\n",
    "def create_words_txt(fname='exp/words.txt', dic_fname='timit/TIMITDIC.TXT'):\n",
    "    fname = Path(fname)\n",
    "    fname.parent.mkdir(exist_ok=True, parents=True)\n",
    "    words = ['<eps>'] + [w for w, _ in sorted(load_lexicon_file(dic_fname))] + ['<s>', '</s>']\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write(''.join(f'{w} {i}\\n' for i, w in enumerate(words)))\n",
    "create_words_txt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876b2c09-14f1-4c5a-aa8c-c7c0efb6d9fe",
   "metadata": {},
   "source": [
    "# H transducer\n",
    "\n",
    "Создадим первый wfst. \n",
    "\n",
    "В гибридном пайплайне задача первых двух wfst (H и C) - это перевести предсказанные с помощью акустической модели классы в последовательность фонем. В нашем случае все значительно легче, тк как АМ (из работы №3) уже учится предсказывать фонемы. H.wfst остается только преобразовать вероятност, которые оценивает AM, в правдоподобие, добавив к скорам фонем их априорную вероятность. \n",
    "\n",
    "\n",
    "Необходимый нам для декодирования граф H.wfst состоит из одного состояния и N петель, где N - это количество фонем. Каждая петля соответствует определенной фонеме и хранит ее логарифм априорной вероятности.\n",
    "\n",
    "![image](resources/lab4/H_example.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68a948bb-4f78-4b4e-9d6e-de4d39ebb1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "572a9a8b-8973-4453-91d9-203a18dd7993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"5753pt\" height=\"189pt\"\n",
       " viewBox=\"0.00 0.00 5753.00 188.56\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184.56)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-184.56 5749,-184.56 5749,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90.31\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.24,-91.81C64.02,-91.84 72,-91.34 72,-90.31 72,-89.68 69.04,-89.25 64.51,-89.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.31,-85.51 54.24,-88.81 64.17,-92.51 64.31,-85.51\"/>\n",
       "<text text-anchor=\"middle\" x=\"121\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">pau:pau:&#45;2.07</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.91,-92.4C96.65,-94.11 170,-93.41 170,-90.31 170,-87.45 107.4,-86.63 64.16,-87.87\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.78,-84.38 53.91,-88.22 64.02,-91.38 63.78,-84.38\"/>\n",
       "<text text-anchor=\"middle\" x=\"219\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">pau:pau:&#45;4.39</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.07,-92.8C119.16,-96.15 268,-95.32 268,-90.31 268,-85.55 133.34,-84.57 64.18,-87.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.9,-83.87 54.07,-87.82 64.22,-90.86 63.9,-83.87\"/>\n",
       "<text text-anchor=\"middle\" x=\"307.5\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">sh:sh:&#45;4.00</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.67,-93.16C133.6,-98.16 347,-97.21 347,-90.31 347,-83.69 150.34,-82.55 63.89,-86.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.46,-83.41 53.67,-87.46 63.85,-90.4 63.46,-83.41\"/>\n",
       "<text text-anchor=\"middle\" x=\"382.5\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">ix:ix:&#45;3.46</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.76,-93.5C146.09,-100.17 418,-99.1 418,-90.31 418,-81.82 164.36,-80.54 63.84,-86.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.51,-82.98 53.76,-87.12 63.96,-89.96 63.51,-82.98\"/>\n",
       "<text text-anchor=\"middle\" x=\"458\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">hv:hv:&#45;5.19</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.69,-93.76C158.61,-102.17 498,-101.02 498,-90.31 498,-79.93 178.85,-78.53 63.85,-86.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.41,-82.65 53.69,-86.86 63.92,-89.63 63.41,-82.65\"/>\n",
       "<text text-anchor=\"middle\" x=\"538\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">eh:eh:&#45;3.70</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.52,-93.98C170.01,-104.18 578,-102.96 578,-90.31 578,-78.01 192.01,-76.52 63.72,-85.83\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.22,-82.36 53.52,-86.64 63.77,-89.34 63.22,-82.36\"/>\n",
       "<text text-anchor=\"middle\" x=\"622\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">dcl:dcl:&#45;4.03</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.44,-94.16C181.81,-106.19 666,-104.91 666,-90.31 666,-76.07 205.16,-74.51 63.47,-85.61\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.11,-82.12 53.44,-86.46 63.71,-89.1 63.11,-82.12\"/>\n",
       "<text text-anchor=\"middle\" x=\"702.5\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">jh:jh:&#45;5.30</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.65,-94.38C192.11,-108.19 739,-106.84 739,-90.31 739,-74.18 217.44,-72.5 64.05,-85.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.29,-81.85 53.65,-86.25 63.93,-88.82 63.29,-81.85\"/>\n",
       "<text text-anchor=\"middle\" x=\"775.5\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">ih:ih:&#45;3.57</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.43,-94.53C200.92,-110.2 812,-108.79 812,-90.31 812,-72.23 226.9,-70.5 63.57,-85.1\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.04,-81.64 53.43,-86.09 63.72,-88.61 63.04,-81.64\"/>\n",
       "<text text-anchor=\"middle\" x=\"844\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">d:d:&#45;5.16</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.66,-94.74C209.83,-112.2 876,-110.72 876,-90.31 876,-70.31 236.23,-68.49 63.64,-84.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.24,-81.38 53.66,-85.89 63.96,-88.35 63.24,-81.38\"/>\n",
       "<text text-anchor=\"middle\" x=\"916\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">ah:ah:&#45;4.24</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.33,-94.84C218.43,-114.2 956,-112.69 956,-90.31 956,-68.36 246.25,-66.49 63.37,-84.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.89,-81.23 53.33,-85.79 63.65,-88.19 62.89,-81.23\"/>\n",
       "<text text-anchor=\"middle\" x=\"999\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">kcl:kcl:&#45;3.70</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.28,-94.95C228.07,-116.21 1042,-114.66 1042,-90.31 1042,-66.4 257.22,-64.48 63.37,-84.55\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.83,-81.08 53.28,-85.68 63.61,-88.04 62.83,-81.08\"/>\n",
       "<text text-anchor=\"middle\" x=\"1073\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">k:k:&#45;4.04</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.28,-95.09C235.55,-118.21 1104,-116.62 1104,-90.31 1104,-64.46 264.98,-62.48 63.21,-84.37\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.8,-80.89 53.28,-85.53 63.61,-87.85 62.8,-80.89\"/>\n",
       "<text text-anchor=\"middle\" x=\"1134.5\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">s:s:&#45;2.82</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.1,-95.2C242.28,-120.21 1165,-118.58 1165,-90.31 1165,-62.51 272.66,-60.47 63.12,-84.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.6,-80.74 53.1,-85.43 63.45,-87.69 62.6,-80.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"1205\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">ux:ux:&#45;4.23</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.23,-95.32C251.43,-122.21 1245,-120.55 1245,-90.31 1245,-60.56 283.19,-58.47 63.42,-84.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.72,-80.59 53.23,-85.31 63.59,-87.54 62.72,-80.59\"/>\n",
       "<text text-anchor=\"middle\" x=\"1277\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">q:q:&#45;4.11</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.08,-95.41C258.25,-124.22 1309,-122.52 1309,-90.31 1309,-58.61 290.83,-56.47 63.31,-83.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.54,-80.46 53.08,-85.22 63.44,-87.4 62.54,-80.46\"/>\n",
       "<text text-anchor=\"middle\" x=\"1349\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">en:en:&#45;5.51</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.91,-95.47C266.08,-126.22 1389,-124.5 1389,-90.31 1389,-56.62 298.74,-54.46 62.86,-83.82\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.36,-80.36 52.91,-85.15 63.29,-87.3 62.36,-80.36\"/>\n",
       "<text text-anchor=\"middle\" x=\"1433\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">gcl:gcl:&#45;4.85</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.99,-95.56C275.19,-128.22 1477,-126.48 1477,-90.31 1477,-54.66 308.99,-52.46 63.01,-83.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.43,-80.25 52.99,-85.07 63.37,-87.18 62.43,-80.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"1509\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">g:g:&#45;5.46</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.15,-95.67C282.8,-130.22 1541,-128.44 1541,-90.31 1541,-52.71 316.97,-50.46 63.14,-83.55\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.57,-80.1 53.15,-84.95 63.54,-87.03 62.57,-80.1\"/>\n",
       "<text text-anchor=\"middle\" x=\"1570.5\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">r:r:&#45;3.58</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.83,-95.72C288.29,-132.23 1600,-130.43 1600,-90.31 1600,-50.75 323.92,-48.45 63.09,-83.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.23,-80.02 52.83,-84.91 63.23,-86.95 62.23,-80.02\"/>\n",
       "<text text-anchor=\"middle\" x=\"1635\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">w:w:&#45;4.22</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.86,-95.8C295.71,-134.23 1670,-132.4 1670,-90.31 1670,-48.78 331.71,-46.45 63.05,-83.33\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.24,-79.91 52.86,-84.82 63.26,-86.83 62.24,-79.91\"/>\n",
       "<text text-anchor=\"middle\" x=\"1709.5\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">ao:ao:&#45;3.66</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.85,-95.86C303.5,-136.23 1749,-134.38 1749,-90.31 1749,-46.8 339.97,-44.45 62.97,-83.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.22,-79.82 52.85,-84.76 63.26,-86.74 62.22,-79.82\"/>\n",
       "<text text-anchor=\"middle\" x=\"1789\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">dx:dx:&#45;5.20</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.76,-95.91C310.95,-138.23 1829,-136.37 1829,-90.31 1829,-44.82 347.79,-42.44 62.78,-83.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.12,-79.74 52.76,-84.71 63.18,-86.66 62.12,-79.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"1875\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">axr:axr:&#45;3.93</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.65,-95.94C318.9,-140.24 1921,-138.36 1921,-90.31 1921,-42.83 356.23,-40.44 62.56,-83.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.99,-79.69 52.65,-84.68 63.07,-86.61 61.99,-79.69\"/>\n",
       "<text text-anchor=\"middle\" x=\"1948.5\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">l:l:&#45;3.70</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.71,-96.03C325.3,-142.24 1976,-140.33 1976,-90.31 1976,-40.88 363.76,-38.44 62.86,-82.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.04,-79.58 52.71,-84.6 63.13,-86.49 62.04,-79.58\"/>\n",
       "<text text-anchor=\"middle\" x=\"2007\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">y:y:&#45;4.83</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.75,-96.1C332.03,-144.24 2038,-142.31 2038,-90.31 2038,-38.9 370.13,-36.44 62.7,-82.92\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.07,-79.48 52.75,-84.52 63.18,-86.39 62.07,-79.48\"/>\n",
       "<text text-anchor=\"middle\" x=\"2079\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">uh:uh:&#45;5.85</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.83,-96.16C339.96,-146.24 2120,-144.29 2120,-90.31 2120,-36.94 379.72,-34.44 63.06,-82.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.13,-79.4 52.83,-84.46 63.26,-86.31 62.13,-79.4\"/>\n",
       "<text text-anchor=\"middle\" x=\"2152\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">n:n:&#45;3.61</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.78,-96.21C346.36,-148.24 2184,-146.27 2184,-90.31 2184,-34.95 385.63,-32.43 62.77,-82.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.07,-79.32 52.78,-84.41 63.22,-86.23 62.07,-79.32\"/>\n",
       "<text text-anchor=\"middle\" x=\"2223.5\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">ae:ae:&#45;3.17</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.74,-96.25C353.54,-150.24 2263,-148.26 2263,-90.31 2263,-32.99 394.34,-30.43 62.98,-82.65\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.02,-79.26 52.74,-84.37 63.18,-86.17 62.02,-79.26\"/>\n",
       "<text text-anchor=\"middle\" x=\"2299.5\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">m:m:&#45;4.08</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.62,-96.28C360.05,-152.24 2336,-150.25 2336,-90.31 2336,-30.99 400.37,-28.43 62.6,-82.64\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.89,-79.21 52.62,-84.34 63.07,-86.11 61.89,-79.21\"/>\n",
       "<text text-anchor=\"middle\" x=\"2375.5\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">oy:oy:&#45;4.86</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge32\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.48,-96.3C366.69,-154.25 2415,-152.25 2415,-90.31 2415,-28.99 407.49,-26.42 62.43,-82.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.74,-79.17 52.48,-84.32 62.93,-86.07 61.74,-79.17\"/>\n",
       "<text text-anchor=\"middle\" x=\"2454.5\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">ax:ax:&#45;4.40</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge33\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.78,-96.4C375.18,-156.25 2494,-154.22 2494,-90.31 2494,-27.03 416.36,-24.42 62.73,-82.49\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.03,-79.06 52.78,-84.23 63.23,-85.96 62.03,-79.06\"/>\n",
       "<text text-anchor=\"middle\" x=\"2535\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">dh:dh:&#45;4.89</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge34\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.55,-96.39C381.51,-158.25 2576,-156.22 2576,-90.31 2576,-25.03 423.1,-22.42 62.45,-82.48\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.79,-79.04 52.55,-84.23 63.01,-85.94 61.79,-79.04\"/>\n",
       "<text text-anchor=\"middle\" x=\"2617\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">tcl:tcl:&#45;3.66</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge35\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.54,-96.43C388.69,-160.25 2658,-158.21 2658,-90.31 2658,-23.04 430.6,-20.42 62.39,-82.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.77,-78.99 52.54,-84.2 63,-85.88 61.77,-78.99\"/>\n",
       "<text text-anchor=\"middle\" x=\"2693.5\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">iy:iy:&#45;3.11</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge36\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.46,-96.45C395.01,-162.25 2729,-160.2 2729,-90.31 2729,-21.07 438.12,-18.42 62.5,-82.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.67,-78.95 52.46,-84.17 62.92,-85.84 61.67,-78.95\"/>\n",
       "<text text-anchor=\"middle\" x=\"2760\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">v:v:&#45;4.77</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge37\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.58,-96.52C401.84,-164.25 2791,-162.18 2791,-90.31 2791,-19.09 444.81,-16.42 62.54,-82.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.79,-78.86 52.58,-84.1 63.05,-85.75 61.79,-78.86\"/>\n",
       "<text text-anchor=\"middle\" x=\"2819\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">f:f:&#45;4.13</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge38\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.4,-96.53C407.01,-166.25 2847,-164.18 2847,-90.31 2847,-17.11 450.89,-14.42 62.52,-82.22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.6,-78.83 52.4,-84.09 62.87,-85.71 61.6,-78.83\"/>\n",
       "<text text-anchor=\"middle\" x=\"2876\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">t:t:&#45;4.21</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge39\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.47,-96.59C413.38,-168.25 2905,-166.16 2905,-90.31 2905,-15.13 456.99,-12.41 62.47,-82.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.65,-78.75 52.47,-84.03 62.94,-85.64 61.65,-78.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"2949\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">pcl:pcl:&#45;4.34</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge40\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.58,-96.64C421.26,-170.25 2993,-168.14 2993,-90.31 2993,-13.15 465.03,-10.41 62.51,-82.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.76,-78.69 52.58,-83.98 63.05,-85.57 61.76,-78.69\"/>\n",
       "<text text-anchor=\"middle\" x=\"3036\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">ow:ow:&#45;3.96</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge41\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.37,-96.63C427.55,-172.25 3079,-170.15 3079,-90.31 3079,-11.16 472.67,-8.41 62.49,-82.08\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.55,-78.69 52.37,-84 62.85,-85.57 61.55,-78.69\"/>\n",
       "<text text-anchor=\"middle\" x=\"3120\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">hh:hh:&#45;5.43</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge42\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.42,-96.66C434.78,-174.26 3161,-172.14 3161,-90.31 3161,-9.17 479.85,-6.41 62.42,-82.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.58,-78.64 52.42,-83.96 62.9,-85.52 61.58,-78.64\"/>\n",
       "<text text-anchor=\"middle\" x=\"3200.5\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">ch:ch:&#45;5.30</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge43\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.43,-96.7C441.7,-176.26 3240,-174.13 3240,-90.31 3240,-7.17 486.61,-4.41 62.3,-82.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.58,-78.59 52.43,-83.93 62.91,-85.47 61.58,-78.59\"/>\n",
       "<text text-anchor=\"middle\" x=\"3284\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">bcl:bcl:&#45;4.77</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge44\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.43,-96.72C448.93,-178.26 3328,-176.12 3328,-90.31 3328,-5.19 495.14,-2.41 62.5,-81.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.58,-78.56 52.43,-83.9 62.91,-85.43 61.58,-78.56\"/>\n",
       "<text text-anchor=\"middle\" x=\"3360\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">b:b:&#45;5.92</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge45\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.37,-96.74C454.84,-180.26 3392,-178.11 3392,-90.31 3392,-3.2 500.56,-0.4 62.26,-81.94\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.51,-78.52 52.37,-83.88 62.85,-85.39 61.51,-78.52\"/>\n",
       "<text text-anchor=\"middle\" x=\"3431.5\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">aa:aa:&#45;3.63</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge46\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.31,-96.76C461.35,-182.26 3471,-180.11 3471,-90.31 3471,-1.21 508.2,1.6 62.37,-81.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.44,-78.49 52.31,-83.87 62.8,-85.36 61.44,-78.49\"/>\n",
       "<text text-anchor=\"middle\" x=\"3515.5\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">em:em:&#45;7.25</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge47\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.24,-96.77C468.19,-184.26 3560,-182.11 3560,-90.31 3560,0.79 514.81,3.6 62.15,-81.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.37,-78.47 52.24,-83.86 62.74,-85.34 61.37,-78.47\"/>\n",
       "<text text-anchor=\"middle\" x=\"3601\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">ng:ng:&#45;5.15</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge48\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.46,-96.84C476.13,-186.26 3642,-184.09 3642,-90.31 3642,2.75 523.87,5.6 62.55,-81.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.58,-78.4 52.46,-83.79 62.95,-85.26 61.58,-78.4\"/>\n",
       "<text text-anchor=\"middle\" x=\"3681.5\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">ay:ay:&#45;3.72</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge49\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.34,-96.84C482.24,-188.26 3721,-186.09 3721,-90.31 3721,4.76 529.51,7.6 62.23,-81.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.45,-78.38 52.34,-83.79 62.83,-85.24 61.45,-78.38\"/>\n",
       "<text text-anchor=\"middle\" x=\"3759\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">th:th:&#45;5.33</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge50\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.19,-96.83C488.08,-190.26 3797,-188.09 3797,-90.31 3797,6.75 536.37,9.6 62.23,-81.76\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.29,-78.38 52.19,-83.8 62.69,-85.24 61.29,-78.38\"/>\n",
       "<text text-anchor=\"middle\" x=\"3849\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">ax&#45;h:ax&#45;h:&#45;7.04</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge51\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.39,-96.89C496.68,-192.26 3901,-190.07 3901,-90.31 3901,8.74 544.72,11.6 62.27,-81.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.49,-78.31 52.39,-83.74 62.89,-85.17 61.49,-78.31\"/>\n",
       "<text text-anchor=\"middle\" x=\"3940.5\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">ey:ey:&#45;3.89</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge52\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.21,-96.87C502.39,-194.26 3980,-192.08 3980,-90.31 3980,10.73 551.46,13.6 62.22,-81.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.3,-78.32 52.21,-83.75 62.7,-85.17 61.3,-78.32\"/>\n",
       "<text text-anchor=\"middle\" x=\"4012\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">p:p:&#45;4.82</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge53\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.32,-96.92C509.09,-196.26 4044,-194.06 4044,-90.31 4044,12.73 557.25,15.61 62.12,-81.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.41,-78.25 52.32,-83.7 62.82,-85.11 61.41,-78.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"4087\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">aw:aw:&#45;4.79</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge54\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.45,-96.97C516.65,-198.26 4130,-196.04 4130,-90.31 4130,14.7 565.88,17.61 62.41,-81.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.54,-78.2 52.45,-83.65 62.96,-85.05 61.54,-78.2\"/>\n",
       "<text text-anchor=\"middle\" x=\"4168\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">er:er:&#45;4.08</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge55\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.21,-96.94C521.89,-200.27 4206,-198.06 4206,-90.31 4206,16.7 572.08,19.61 62.29,-81.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.28,-78.21 52.21,-83.68 62.71,-85.07 61.28,-78.21\"/>\n",
       "<text text-anchor=\"middle\" x=\"4246\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">nx:nx:&#45;6.23</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge56\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.31,-96.98C529.02,-202.27 4286,-200.04 4286,-90.31 4286,18.7 578.38,21.61 62.16,-81.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.37,-78.16 52.31,-83.64 62.81,-85.01 61.37,-78.16\"/>\n",
       "<text text-anchor=\"middle\" x=\"4316.5\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">z:z:&#45;3.80</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge57\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.37,-97.02C535.32,-204.27 4347,-202.03 4347,-90.31 4347,20.67 585.41,23.61 62.35,-81.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.43,-78.11 52.37,-83.6 62.88,-84.96 61.43,-78.11\"/>\n",
       "<text text-anchor=\"middle\" x=\"4382.5\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">el:el:&#45;5.10</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge58\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.43,-97.06C541.97,-206.27 4418,-204.02 4418,-90.31 4418,22.64 592.9,25.61 62.55,-81.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.49,-78.07 52.43,-83.57 62.94,-84.92 61.49,-78.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"4462\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">uw:uw:&#45;5.48</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge59\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.14,-97.01C547.24,-208.27 4506,-206.04 4506,-90.31 4506,24.68 597.34,27.61 61.98,-81.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.18,-78.1 52.14,-83.62 62.65,-84.95 61.18,-78.1\"/>\n",
       "<text text-anchor=\"middle\" x=\"4545.5\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">zh:zh:&#45;7.05</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge60\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.18,-97.04C554.02,-210.27 4585,-208.03 4585,-90.31 4585,26.66 605.04,29.61 62.16,-81.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.22,-78.07 52.18,-83.59 62.69,-84.91 61.22,-78.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"4634\" y=\"-86.61\" font-family=\"Times,serif\" font-size=\"14.00\">eng:eng:&#45;8.42</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7ff53cbe3be0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_h_wfst(prior_file='resources/lab4/phone.priors', phones_fname='exp/phones.txt'):\n",
    "    \"\"\"Create H.wfst. just transduce AM probability to AM likelihood (Bayes theorem)\n",
    "    \"\"\"\n",
    "    symbols = SymbolsMap.from_file(phones_fname)\n",
    "    h_fst = WFST(isymbols=symbols, osymbols=symbols)\n",
    "    s = h_fst.get_start()\n",
    "    h_fst.set_final(s)\n",
    "    with open(prior_file) as f:\n",
    "        for ph, prior in map(str.split, f.readlines()):\n",
    "            log_prior = np.log(float(prior))\n",
    "            ph_id = symbols.get_id(ph)\n",
    "            h_fst.add_arc(s, Arc(ph_id, ph_id, log_prior, s))\n",
    "    return h_fst\n",
    "create_h_wfst().to_dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5765ec-0a60-4dc1-9bdd-28975b45f971",
   "metadata": {},
   "source": [
    "# L transducer\n",
    "\n",
    "Lwfst - это WFST, который преобразует входные последовательности фонем в слова.\n",
    "\n",
    "Например, `pau pau ae1 ae1 ae1 ae1 ae1 ae1 r pau pau pau y y y y eh1 s s s pau` -> `air yes`\n",
    "\n",
    "Чтобы контролировать количество слов в выходной гипотезе, в L добавляется два веса: \n",
    "* word_insertion_penalty - штраф за добавление слова \n",
    "* stay_in_silence_penalty - штраф за пропуск паузы (фонемы pau)\n",
    "\n",
    "Пример Lwfst, построенного для слов `'em` `-knacks`, с параметрами word_insertion_penalty=0.5, stay_in_silence_penalty=0.01\n",
    "\n",
    "![image](./resources/lab4/L_example.svg)\n",
    "\n",
    "Фонема паузы pau обрабатывается особым способом: L.fst может пропускать ее, не генерируя ничего на выходе. Также у окончания каждой транскрипции слова есть две отдельные дуги, возвращающие FST в стартовое состояние. Это нужно для того, чтобы была возможность обработать как подряд дупликаты последней фонемы на конце слова, так и перейти в стартовое состояние всего по одной финальной фонеме. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cf8a345f-aee0-44a6-a0f6-4346595f5d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"691pt\" height=\"392pt\"\n",
       " viewBox=\"0.00 0.00 690.93 392.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 388)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-388 686.93,-388 686.93,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"280.93\" cy=\"-366\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"280.93\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M305.47,-373.75C316.44,-374.49 325.93,-371.91 325.93,-366 325.93,-362.03 321.65,-359.56 315.51,-358.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"315.58,-355.09 305.47,-358.25 315.34,-362.09 315.58,-355.09\"/>\n",
       "<text text-anchor=\"middle\" x=\"383.93\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\">pau:&lt;eps&gt;:0.01</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"39.93\" cy=\"-279\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"39.93\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M253.8,-363.58C187.29,-359.81 20.32,-348.59 3.93,-330 -4.66,-320.25 3.01,-308.49 13.45,-298.83\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"15.92,-301.32 21.34,-292.22 11.43,-295.95 15.92,-301.32\"/>\n",
       "<text text-anchor=\"middle\" x=\"56.93\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">ax:&lt;eps&gt;:0.00</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"399.93\" cy=\"-279\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"399.93\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">3</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>0&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M303.34,-355.54C317.09,-349.25 334.72,-340.26 348.93,-330 359.93,-322.06 370.84,-311.78 379.73,-302.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"382.36,-304.91 386.68,-295.23 377.27,-300.11 382.36,-304.91\"/>\n",
       "<text text-anchor=\"middle\" x=\"416.43\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">n:&lt;eps&gt;:0.00</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;0 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>1&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M61.4,-289.95C75.43,-296.64 94.02,-305.87 109.93,-315 120.52,-321.07 121.72,-325.19 132.93,-330 169.29,-345.59 213.79,-354.92 244.44,-359.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"244.25,-363.49 254.67,-361.59 245.34,-356.58 244.25,-363.49\"/>\n",
       "<text text-anchor=\"middle\" x=\"172.93\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\">m:&#39;em:0.50</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;1 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>1&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M64.47,-286.75C75.44,-287.49 84.93,-284.91 84.93,-279 84.93,-275.03 80.65,-272.56 74.51,-271.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"74.58,-268.09 64.47,-271.25 74.34,-275.09 74.58,-268.09\"/>\n",
       "<text text-anchor=\"middle\" x=\"137.93\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">ax:&lt;eps&gt;:0.00</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"133.93\" cy=\"-192\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"133.93\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M55.46,-263.96C70.56,-250.31 93.69,-229.39 110.93,-213.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"113.32,-216.36 118.39,-207.06 108.62,-211.17 113.32,-216.36\"/>\n",
       "<text text-anchor=\"middle\" x=\"144.43\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">m:&lt;eps&gt;:0.00</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;0 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>2&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M156.98,-201.49C170.65,-207.38 187.64,-216.27 199.93,-228 200.52,-228.56 243.26,-301.05 266.41,-340.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"263.48,-342.26 271.57,-349.1 269.51,-338.71 263.48,-342.26\"/>\n",
       "<text text-anchor=\"middle\" x=\"280.93\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">m:&#39;em:0.50</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;2 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>2&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.47,-199.75C169.44,-200.49 178.93,-197.91 178.93,-192 178.93,-188.03 174.65,-185.56 168.51,-184.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"168.58,-181.09 158.47,-184.25 168.34,-188.09 168.58,-181.09\"/>\n",
       "<text text-anchor=\"middle\" x=\"230.43\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">m:&lt;eps&gt;:0.00</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;3 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>3&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M424.47,-286.75C435.44,-287.49 444.93,-284.91 444.93,-279 444.93,-275.03 440.65,-272.56 434.51,-271.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"434.58,-268.09 424.47,-271.25 434.34,-275.09 434.58,-268.09\"/>\n",
       "<text text-anchor=\"middle\" x=\"494.43\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">n:&lt;eps&gt;:0.00</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"393.93\" cy=\"-192\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"393.93\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">4</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M419.04,-265.98C431.7,-256.39 444.8,-242.24 437.93,-228 434.55,-220.99 429.01,-214.95 422.97,-209.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"424.73,-206.91 414.6,-203.8 420.59,-212.55 424.73,-206.91\"/>\n",
       "<text text-anchor=\"middle\" x=\"491.93\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">ae:&lt;eps&gt;:0.00</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;4 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>4&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M418.47,-199.75C429.44,-200.49 438.93,-197.91 438.93,-192 438.93,-188.03 434.65,-185.56 428.51,-184.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"428.58,-181.09 418.47,-184.25 428.34,-188.09 428.58,-181.09\"/>\n",
       "<text text-anchor=\"middle\" x=\"491.93\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">ae:&lt;eps&gt;:0.00</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"393.93\" cy=\"-105\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"393.93\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">5</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M393.93,-173.8C393.93,-162.16 393.93,-146.55 393.93,-133.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"397.43,-133.18 393.93,-123.18 390.43,-133.18 397.43,-133.18\"/>\n",
       "<text text-anchor=\"middle\" x=\"442.93\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">k:&lt;eps&gt;:0.00</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;0 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>5&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M385.18,-122.12C377.77,-135.84 366.97,-156.12 357.93,-174 345.92,-197.78 339.89,-202.58 331.93,-228 322.65,-257.64 331.43,-267.77 320.93,-297 315.34,-312.56 306.13,-328.56 297.96,-341.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"294.92,-339.37 292.25,-349.62 300.74,-343.27 294.92,-339.37\"/>\n",
       "<text text-anchor=\"middle\" x=\"382.93\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">s:&#45;knacks:0.50</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;5 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>5&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M418.47,-112.75C429.44,-113.49 438.93,-110.91 438.93,-105 438.93,-101.03 434.65,-98.56 428.51,-97.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"428.58,-94.09 418.47,-97.25 428.34,-101.09 428.58,-94.09\"/>\n",
       "<text text-anchor=\"middle\" x=\"487.93\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">k:&lt;eps&gt;:0.00</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"487.93\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"487.93\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">6</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M409.46,-89.96C424.56,-76.31 447.69,-55.39 464.93,-39.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"467.32,-42.36 472.39,-33.06 462.62,-37.17 467.32,-42.36\"/>\n",
       "<text text-anchor=\"middle\" x=\"494.43\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">s:&lt;eps&gt;:0.00</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;0 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>6&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M510.92,-28.08C538.46,-40.64 580.93,-66.16 580.93,-104 580.93,-280 580.93,-280 580.93,-280 580.93,-334.07 398.23,-355.52 317.98,-362.33\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"317.5,-358.85 307.82,-363.16 318.07,-365.83 317.5,-358.85\"/>\n",
       "<text text-anchor=\"middle\" x=\"631.93\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">s:&#45;knacks:0.50</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;6 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>6&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M512.47,-25.75C523.44,-26.49 532.93,-23.91 532.93,-18 532.93,-14.03 528.65,-11.56 522.51,-10.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"522.58,-7.09 512.47,-10.25 522.34,-14.09 522.58,-7.09\"/>\n",
       "<text text-anchor=\"middle\" x=\"581.43\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">s:&lt;eps&gt;:0.00</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7ff53aa9ce50>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "def create_Lwfst_from_file(dic_fname='timit/TIMITDIC.TXT', \n",
    "                           phones_fname='exp/phones.txt', \n",
    "                           words_fname='exp/words.txt', \n",
    "                           words_limit=-1,\n",
    "                           word_insertion_penalty=0.5, \n",
    "                           stay_in_silence_penalty=0.01):\n",
    "    isymbols = SymbolsMap.from_file(phones_fname)\n",
    "    osymbols = SymbolsMap.from_file(words_fname)\n",
    "    sil_id = isymbols.get_id('pau')\n",
    "    l_wfst = WFST(isymbols=isymbols, osymbols=osymbols)\n",
    "    start = l_wfst.get_start()\n",
    "    ### Пропуск скольких угодно фонем тишины\n",
    "    l_wfst.add_arc(start, Arc(sil_id, 0, stay_in_silence_penalty, start))\n",
    "    # Стартовое состояние также является и финальным\n",
    "    l_wfst.set_final(start)\n",
    "    \n",
    "    #for word, trans in load_lexicon_file(dic_fname, words_limit=words_limit):\n",
    "        #tran_ids = [isymbols.get_id(t) for t in trans]\n",
    "        #word_id = osymbols.get_id(word)\n",
    "        # TODO\n",
    "        # Добавьте последовательные дуги в wfst, определяющие транскрипцию слова\n",
    "        # Все дуги, кроме последней, на выходе имеют <eps>\n",
    "        # Последняя дуга выводит word_id и возвращает конечный автомат в стартовое (оно же финальное) состояние\n",
    "        # Каждая фонема может тянуться от 1 до inf кадров, поэтому помимо переходов в новое состояние, должны быть еще петли \n",
    "        #eps_id = osymbols.get_id('<eps>')\n",
    "\n",
    "    for word, trans in load_lexicon_file(dic_fname, words_limit=words_limit):\n",
    "        tran_ids: List[int] = [isymbols.get_id(t) for t in trans]\n",
    "        word_id: int = osymbols.get_id(word)\n",
    "        eps_id: int = osymbols.get_id('<eps>')\n",
    "\n",
    "        state: int = start\n",
    "        for idx in range(len(tran_ids) + 1):\n",
    "            if idx < len(tran_ids):\n",
    "                ilabel: int = tran_ids[idx]\n",
    "                olabel: int = eps_id\n",
    "                weight: float = 0.0\n",
    "                nextstate: int = l_wfst.new_state()\n",
    "\n",
    "                arc = Arc(ilabel, olabel, weight, nextstate)\n",
    "                l_wfst.add_arc(state, arc)\n",
    "\n",
    "                loop = Arc(ilabel, eps_id, 0.0, nextstate)\n",
    "                l_wfst.add_arc(nextstate, loop)\n",
    "            \n",
    "                if idx < len(tran_ids) - 1:\n",
    "                    state = nextstate\n",
    "            else:\n",
    "                arc = Arc(ilabel, word_id, word_insertion_penalty, start)\n",
    "                l_wfst.add_arc(state, arc)\n",
    "\n",
    "                arc_to_start = Arc(ilabel, word_id, word_insertion_penalty, start)\n",
    "                l_wfst.add_arc(nextstate, arc_to_start)\n",
    "\n",
    "\n",
    "\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "    return l_wfst\n",
    "create_Lwfst_from_file(words_limit=2, word_insertion_penalty=0.5, stay_in_silence_penalty=0.01).to_dot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e518300-0337-4727-9312-c2027b73cb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|█████████████████████████████████████▎                                           | 46/100 [00:00<00:00, 436.81it/s]"
     ]
    }
   ],
   "source": [
    "def test_l_wfst():\n",
    "    l_wfst = create_Lwfst_from_file(dic_fname='timit/TIMITDIC.TXT', \n",
    "                                    word_insertion_penalty=1.0, stay_in_silence_penalty=0.01)\n",
    "    start = l_wfst.get_start()\n",
    "    hyps = l_wfst.transduce(start, 'pau')\n",
    "    loop_hyp = [(l, w, s) for l, w, s in hyps if s == start]\n",
    "    assert (len(loop_hyp) == 1 \n",
    "            and loop_hyp[0][0] == '<eps>' \n",
    "            and loop_hyp[0][1] == 0.01), f\"{hyps}\\n skip pause test failed\"\n",
    "    # смотрим, что поданная на вход транскрипция корректно переводится в слово\n",
    "    # количество подряд идущих одинаковых фонем постепенно увеличивается от одной до 6\n",
    "    for i, (word, trans) in enumerate(tqdm(load_lexicon_file('timit/TIMITDIC.TXT', words_limit=100), total=100)):\n",
    "        hyps = [('', 0, l_wfst.get_start()), ]\n",
    "        # Breadth First Search\n",
    "        for t in [t for t in trans for _ in range(i//20+1)]:\n",
    "            # t repeated many times\n",
    "            new_hyps = []\n",
    "            for prev_l, prev_w, prev_s in hyps: \n",
    "                new_hyps.extend([(f\"{prev_l} {l}\", prev_w + w, s) for l,w,s in l_wfst.transduce(prev_s, t)])\n",
    "            hyps = new_hyps\n",
    "        # choose only final hyp\n",
    "        # ограничение веса нужно, чтобы отсечь гипотезы где комбинация других слов мапится на теже фонемы\n",
    "        hyps = [(l, w, s) for l, w, s in hyps if l_wfst.final_score(s) != float('inf') and w == 1.0]\n",
    "       \n",
    "        assert len(hyps) == 1 or len(trans) == 1 , f\"Test failed for word \\\"{word}\\\" {hyps=}\"\n",
    "        assert hyps[0][0].replace('<eps>', ' ').split() == [word], f\"Test failed for word {word} {hyps=}\"\n",
    "        assert hyps[0][1] == 1.0, f\"Test failed for word {word} {hyps=}\"\n",
    "        assert hyps[0][2] == start, f\"Test failed for word {word} {hyps=}\"       \n",
    "    print(\"test 2.a passed\")\n",
    "test_l_wfst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02058420-ef3d-4904-9ceb-c4f80cf17b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca25da4d-9f98-4fb3-adfb-2a1c21c65286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4085d806-f619-4941-b339-6c7ee21b3803",
   "metadata": {},
   "source": [
    "# Поиск наилучшей гипотезы распознавания с помощью beam search\n",
    "\n",
    "Для того, чтобы найти результат распознавания с помощью акустической модели и графа распознавания, надо выполнить поиск наилучшей гипотезы в графе. Для поиска лучшего пути будем использовать [лучевой поиск (Beam Search)](https://ru.wikipedia.org/wiki/%D0%9B%D1%83%D1%87%D0%B5%D0%B2%D0%BE%D0%B9_%D0%BF%D0%BE%D0%B8%D1%81%D0%BA). В основе этого алгоритма лежит обход графа в ширину, но на каждом шаге рассматриваются только наиболее \"хорошие\" гипотезы, а остальные уничтожаются. Такая эвристика позволяет значительно ускорить процесс декодирования, но не всегда находит наилучший путь. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "746bf3bb-9301-423e-9464-0479a9435f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(order=True)\n",
    "class Hypothesis:\n",
    "    # накопленный штраф гипотезы\n",
    "    score: float\n",
    "    # до какого кадра из входной последовательности дошла гипотеза\n",
    "    time: int\n",
    "    # накопленная последовательность слов\n",
    "    words: List[str]\n",
    "    # состояние графа декодирования \n",
    "    state: Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2142b823-9612-46ee-a54b-152b37d815ff",
   "metadata": {},
   "source": [
    "### HypothesesKeeper\n",
    "Реализуем специальную коллекцию, которая хранит все гипотезы распознавания и применяет прунинг (удаление ненужных гипотез). \n",
    "Прунинг делится на два типа: \n",
    "#### State pruning \n",
    "Eсли мы можем дойти до состояния X в момент времени T несколькими способами, то для дальнейшей обработки достаточно только наилучшего пути до состояния X. Эта гипотеза гарантированно будет лучше всех других гипотез, проходящих через точку (X, T). \n",
    "#### Beam pruning\n",
    "Эвристика лучевого поиска. Отсекаем все гипотезы, которые хуже, чем лучшая текущая гипотеза плюс beam_size. Вероятность того, что гипотезы с сильно худшим весом вдруг станут наилучшими, крайне мала, поэтому такая эвристика работает достаточно хорошо. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "58a2485a-834f-499c-a356-b17983fd85fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypothesesKeeper:\n",
    "    def __init__(self, init_hyps: List[Hypothesis] = [], beam_size=10):\n",
    "        self.state2hyp = {h.state: h for h in init_hyps}\n",
    "        self.beam_size = beam_size\n",
    "        if len(self.state2hyp) == 0:\n",
    "            self.set_best(None, float('inf'))\n",
    "        else:\n",
    "            min_hyp = min(self.state2hyp.values())\n",
    "            self.set_best(min_hyp.state, min_hyp.score)\n",
    "\n",
    "    def set_best(self, state, score):\n",
    "        self.best_state = state\n",
    "        self.best_score = score\n",
    "        \n",
    "    def get_best_hyp(self) -> Hypothesis:\n",
    "        \"\"\"return the best hyp based on self.best_state\"\"\"   \n",
    "        # TODO \n",
    "        # верните лучшую гипотезу \n",
    "        #raise NotImplementedError()\n",
    "        # !!!!!!!!!!\n",
    "        return self.state2hyp.get(self.best_state)\n",
    "    \n",
    "    def prune(self):\n",
    "        self.state2hyp = {s:h for s, h in self.state2hyp.items() if not self.is_prunned_by_beam(h.score)}\n",
    "        \n",
    "    def tolist(self) -> List[Hypothesis]:\n",
    "        \"\"\"Return all hypotheses. Apply beam pruning\"\"\"\n",
    "        # TODO \n",
    "        # верните все гипотезы, которые находятся в луче поиска \n",
    "        #raise NotImplementedError()\n",
    "        # !!!!!!!!!!!!!!!!!\n",
    "        return [h for h in self.state2hyp.values() if not self.is_prunned_by_beam(h.score)]\n",
    "    \n",
    "    def is_prunned_by_beam(self, score: float):\n",
    "        \"\"\"Return true if score greater than beam\"\"\"\n",
    "        # TODO \n",
    "        # верните True, если значение score находится вне луча поиска \n",
    "        #raise NotImplementedError()\n",
    "        #!!!!!!!!!!!!!!!!\n",
    "        return score > self.best_score + self.beam_size\n",
    "        \n",
    "    def is_prunned_by_state(self, state, score):\n",
    "        \"\"\"Returns true if the keeper already has a hyp in the same state and the score of this hyp is lower\"\"\"\n",
    "        # TODO\n",
    "        # верните True, если state уже имеет гипотезу, со скором лучше чем score\n",
    "        #raise NotImplementedError()\n",
    "        # !!!!!!!!!!!!!!!!!!!\n",
    "        existing_hyp = self.state2hyp.get(state)\n",
    "        if existing_hyp is None:\n",
    "            return False\n",
    "        return score > existing_hyp.score\n",
    "    \n",
    "    def append(self, hyp: Hypothesis):\n",
    "        \"\"\"Append new hyp into collection\"\"\"\n",
    "        if self.is_prunned_by_beam(hyp.score) or self.is_prunned_by_state(hyp.state, hyp.score):\n",
    "            return \n",
    "        self.state2hyp[hyp.state] = hyp\n",
    "        if hyp.score < self.best_score or self.best_state is None:\n",
    "            self.set_best(hyp.state, hyp.score)\n",
    "\n",
    "    def extend(self, hyps):\n",
    "        if isinstance(hyps, HypothesesKeeper):\n",
    "            hyps = hyps.tolist()\n",
    "        for h in hyps:\n",
    "            self.append(h)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.state2hyp)\n",
    "\n",
    "    def __str__(self):\n",
    "        return '[' + \",\\n\".join(map(str, self.tolist())) + ']'\n",
    "\n",
    "    def describe(self):\n",
    "        # TODO \n",
    "        # посчитайте средний и максимальный скор хранящихся гипотез\n",
    "        # mean_score = ...\n",
    "        # max_score = ...\n",
    "        #raise NotImplementedError()\n",
    "        scores = [h.score for h in self.state2hyp.values()]\n",
    "        mean_score = sum(scores) / len(scores) if scores else float('inf')\n",
    "        max_score = max(scores) if scores else float('inf')\n",
    "        # !!!!!!!\n",
    "        return (\n",
    "            f\"{len(self)} hyps. \"\n",
    "            f\"Best {self.get_best_hyp()}. \"\n",
    "            f\"Mean score {mean_score:.2f}. \"\n",
    "            f\"Max {max_score:.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "349961f4-c05b-41e5-a514-277b69bde94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 2.b passed!\n"
     ]
    }
   ],
   "source": [
    "def test_hyp_keeper():\n",
    "    hyps = HypothesesKeeper(beam_size=10)\n",
    "    hyps.append(Hypothesis(10, 0, ['a'],  1))\n",
    "    assert len(hyps) == 1 and hyps.best_state == 1 and hyps.best_score == 10, f\"First append doesn't work well\"\n",
    "    hyps.append(Hypothesis(100, 0, ['err'], 2))\n",
    "    assert len(hyps) == 1 and hyps.best_state == 1 and hyps.best_score == 10, f\"Append didn't prune the input\"\n",
    "    \n",
    "    hyps.append(Hypothesis(11, 0, ['b'], 2))\n",
    "    assert len(hyps) == 2 and hyps.best_state == 1 and hyps.best_score == 10, f\"Append doesn't work well\"\n",
    "    hyps.append(Hypothesis(12, 0, ['err2'], 2))\n",
    "    assert len(hyps) == 2 and hyps.state2hyp[2].score == 11, f\"Append didn't prune the input\"\n",
    "    \n",
    "    hyps.append(Hypothesis(0, 0, ['c'], 3))\n",
    "    assert hyps.best_state == 3 and hyps.best_score == 0, f\"Append didn't update best_* attributes\"\n",
    "\n",
    "    hyps_list = hyps.tolist()\n",
    "    assert len(hyps_list) == 2, f\"tolist didn't prune the output\"\n",
    "\n",
    "    hyp = hyps.get_best_hyp()\n",
    "    assert hyp == Hypothesis(0, 0, ['c'], 3), f\"{hyp=}\"\n",
    "    print(\"test 2.b passed!\")\n",
    "test_hyp_keeper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "972bbcf8-e7db-4670-8a93-f13760cdcd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearcher:\n",
    "    def __init__(self, am2phone_mapping, graph: AbstractWFST, beam_size=3):\n",
    "        self.am2phone_mapping = am2phone_mapping\n",
    "        self.graph = graph\n",
    "        self.beam_size = beam_size\n",
    "        \n",
    "    def decode(self, phonemes_nll: np.ndarray):\n",
    "        \"\"\"\n",
    "        Decoding input phonemes negative loglikelihood into word level hypthesis\n",
    "        phonemes_logprobs.shape is (Time, num_phones)\n",
    "        \"\"\"\n",
    "        parent_hyps = HypothesesKeeper(init_hyps=[Hypothesis(0, -1, [], self.graph.get_start())], \n",
    "                                       beam_size=self.beam_size)\n",
    "        pbar = tqdm(phonemes_nll)\n",
    "        for new_time, frame_dist in enumerate(pbar):\n",
    "            new_hyps = HypothesesKeeper(beam_size=self.beam_size)\n",
    "            # print(parent_hyps)\n",
    "            for parent_hyp in parent_hyps.tolist():\n",
    "                assert parent_hyp.time + 1 == new_time, f\"Wrong time {new_time=}, \\n{parent_hyp=}\"\n",
    "                # TODO \n",
    "                # Продолжите гипотезу parent_hyp с помощью всех фонем и их вероятностей из frame_dist\n",
    "                # соханите новые гипотезы в new_hyps\n",
    "                # words гипотез не должен содержать <eps> \n",
    "                # score гипотезы равен сумме скора родителя, phone_nll и веса от transduce по графу\n",
    "                for i, phone_nll in enumerate(frame_dist):\n",
    "                    phone = self.am2phone_mapping[i]  # Получаем фонему из ее индекса\n",
    "                    for olabel, weight, nextstate in self.graph.transduce(parent_hyp.state, phone):\n",
    "                        if olabel != '<eps>':\n",
    "                            new_words = parent_hyp.words + [olabel]\n",
    "                        else:\n",
    "                            new_words = parent_hyp.words\n",
    "                        new_score = parent_hyp.score + phone_nll + weight\n",
    "                        new_hyp = Hypothesis(new_score, new_time, new_words, nextstate)\n",
    "                        new_hyps.append(new_hyp)\n",
    "                # !!!!!!!!\n",
    "            new_hyps.prune()\n",
    "            parent_hyps = new_hyps\n",
    "            statictic_str = parent_hyps.describe()\n",
    "            pbar.set_description(statictic_str, refresh=False)\n",
    "        # TODO \n",
    "        # сформируйте список финальных гипотез\n",
    "        # пройдитесь по parent_hyps и добавьте ко всем гипотезам финальный вес \n",
    "        final_hyps = HypothesesKeeper(beam_size=self.beam_size)\n",
    "        #raise NotImplementedError()\n",
    "        # !!!!!!!!!!!!!!!\n",
    "        for hyp in parent_hyps.tolist():\n",
    "            final_score = self.graph.final_score(hyp.state)\n",
    "            if final_score != float('inf'):\n",
    "                final_hyp = Hypothesis(hyp.score + final_score, hyp.time, hyp.words, hyp.state)\n",
    "                final_hyps.append(final_hyp)\n",
    "        \n",
    "        print(f\"Found {len(final_hyps)} hypotheses\")\n",
    "        best_hyp = final_hyps.get_best_hyp()\n",
    "        return best_hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "85ecdf4e-0d13-4ea6-b8b2-1826c009ef66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 hyps. Best Hypothesis(score=-96.0, time=2, words=['a', '_', 'a'], state=1). Mean score -96.00. Max -96.00: 100%|█| 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 hypotheses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 hyps. Best Hypothesis(score=207.0, time=2, words=['b', '_', 'a'], state=1). Mean score 207.00. Max 207.00: 100%|█| 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 hypotheses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2 hyps. Best Hypothesis(score=110.0, time=2, words=['a', '_'], state=0). Mean score 112.00. Max 114.00: 100%|█| 3/3 [00:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 hypotheses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2 hyps. Best Hypothesis(score=1110.0, time=2, words=['b', '_', 'a'], state=1). Mean score 1111.50. Max 1113.00: 100%|█| \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 hypotheses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2 hyps. Best Hypothesis(score=97, time=2, words=[], state=0). Mean score 98.50. Max 100.00: 100%|█| 3/3 [00:00<00:00, 24"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 hypotheses\n",
      "Test 2.c passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test_beam_search():\n",
    "    \n",
    "    graph = WFST(isymbols=SymbolsMap(id2symbol=['<eps>', 'pau', 'A', 'B']),\n",
    "                 osymbols=SymbolsMap(id2symbol=['<eps>', 'err', '_', 'a', 'b']))\n",
    "    s = graph.new_state()\n",
    "    graph.add_arc(0, Arc(2, 3, 2.0, s))\n",
    "    graph.add_arc(0, Arc(3, 4, 3.0, s))\n",
    "    graph.add_arc(s, Arc(1, 2, 1.0, 0))\n",
    "    graph.set_final(s)\n",
    "                 \n",
    "    searcher = BeamSearcher(am2phone_mapping={0: 'pau', 1: 'A', 2: 'B'}, graph=graph, beam_size=10)\n",
    "    #                   p  A  B\n",
    "    logits = np.array([[-1, -1, -1],\n",
    "                       [0, 0, 0],\n",
    "                       [-100, -100, -100]])\n",
    "    best_hyp = searcher.decode(logits)\n",
    "    assert best_hyp == Hypothesis((-1+2) + (-0+1) + (-100+2) , 2, ['a', '_', 'a'], s), best_hyp\n",
    "    logits = np.array([[1, 4, 1],\n",
    "                       [100, 0, 0],\n",
    "                       [100, 100, 100]])\n",
    "    best_hyp = searcher.decode(logits)\n",
    "    assert best_hyp == Hypothesis((1+3) + (100+1) + (100+2) , 2, ['b', '_', 'a'], s), best_hyp\n",
    "    \n",
    "    searcher.graph.add_arc(0, Arc(2, 0, 5.0, 0))\n",
    "    logits = np.array([[1, 2, 0],\n",
    "                       [10, 0, 0],\n",
    "                       [100, 100, 100]])\n",
    "    best_hyp = searcher.decode(logits)\n",
    "    assert best_hyp == Hypothesis((2+5) + (0+5) + (100+2) , 2, ['a'], s), best_hyp\n",
    "\n",
    "    logits = np.array([[1, 20, 4], # beam pruning must remove A-loop hypothesis\n",
    "                       [1000, 0, 0],\n",
    "                       [100, 100, 100]])\n",
    "    best_hyp = searcher.decode(logits)\n",
    "    assert best_hyp == Hypothesis((4+3) + (1000+1) + (100+2) , 2, ['b', '_', 'a'], s), best_hyp\n",
    "\n",
    "    searcher.graph.add_arc(0, Arc(3, 0, -1, 0))\n",
    "    logits = np.array([[1, 2, 0],\n",
    "                       [10, 0, 0],\n",
    "                       [100, 100, 100]])\n",
    "    best_hyp = searcher.decode(logits)\n",
    "    assert best_hyp == Hypothesis((0-1) + (0-1) + (100+2) , 2, ['a'], s), best_hyp\n",
    "\n",
    "    print(f\"Test 2.c passed\")\n",
    "test_beam_search()                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c85696-b4fa-487a-845c-c8d0203a4753",
   "metadata": {},
   "source": [
    "### WFST представление нграмной языковой модели \n",
    "Для подсчета языковой модели будем использовать библиотеку kenlm. Данная библиотека позволяет подсчитывать языковую вероятность с помощью нграмной языковой модели. \n",
    "\n",
    "Создадим обертку над kenlm.Model, реализующую интерфейс AbstractWFST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1192ca61-a0c2-43f0-8954-83840989c030",
   "metadata": {},
   "outputs": [],
   "source": [
    "class kenlmLikeWFST(AbstractWFST):\n",
    "    \"\"\"Оборачиваем kenlm в унифицированный интерфейс\"\"\"\n",
    "    def __init__(self, lm: kenlm.Model, lmwt_factor=0.01):\n",
    "        self.lm = lm\n",
    "        self.lmwt_factor = lmwt_factor\n",
    "        \n",
    "    def get_start(self):\n",
    "        state = kenlm.State()\n",
    "        self.lm.BeginSentenceWrite(state)\n",
    "        return state\n",
    "        \n",
    "    def final_score(self, state: kenlm.State):\n",
    "        logprob = self.lm.BaseScore(state, \"</s>\", kenlm.State())\n",
    "        return self._log10_to_nll(logprob)\n",
    "        \n",
    "    def _log10_to_nll(self, logprob):\n",
    "        \"\"\"переводим в negative натуральный логирифм \"\"\"\n",
    "        return - np.log(10**logprob)\n",
    "        \n",
    "    def transduce(self, state: kenlm.State, ilabel: str):\n",
    "        assert isinstance(ilabel, str), ilabel\n",
    "        if ilabel == '<eps>':\n",
    "            # skip <eps> input\n",
    "            return [('<eps>', 0, state), ]\n",
    "        state2 = kenlm.State()\n",
    "        logprob = self.lm.BaseScore(state, ilabel, state2)\n",
    "        return [(ilabel, self.lmwt_factor * self._log10_to_nll(logprob), state2), ]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd124f1a-0232-48c0-933a-ef5247ab46ed",
   "metadata": {},
   "source": [
    "## Запускаем декодирование\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb39324b-a225-4e8d-914a-5dd99e1ddb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заранее подготовленные negative log probability от модели из работы №3\n",
    "with ReadHelper('ark:resources/lab4/test_am_nlogprobs.ark') as am_nlogprob_reader:\n",
    "    am_nlogprobs = {uri: am_logprob  for uri, am_logprob in am_nlogprob_reader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d745f0de-a7e2-4b01-b363-cb984ea4d747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She had your dark suit in greasy wash water all year\n"
     ]
    }
   ],
   "source": [
    "# Выбираем один пример для отладки пайплайна\n",
    "example = am_nlogprobs['timit/data/TEST/DR1/FAKS0/SA1']\n",
    "with open('timit/data/TEST/DR1/FAKS0/SA1.TXT') as f:\n",
    "    example_ref = ' '.join(f.read().replace('.', ' ').split()[2:])\n",
    "print(example_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d3a3d464-41b5-46a4-95bc-d0e4900549cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WFST' object has no attribute 'add_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m h_fst \u001b[38;5;241m=\u001b[39m create_h_wfst()\n\u001b[0;32m----> 2\u001b[0m l_fst \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_Lwfst_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstay_in_silence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_insertion_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 33\u001b[0m, in \u001b[0;36mcreate_Lwfst_from_file\u001b[0;34m(dic_fname, phones_fname, words_fname, words_limit, word_insertion_penalty, stay_in_silence_penalty)\u001b[0m\n\u001b[1;32m     30\u001b[0m current_state \u001b[38;5;241m=\u001b[39m start\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, phone_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tran_ids):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Создание нового состояния для каждой фонемы\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[43ml_wfst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_state\u001b[49m()\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Добавление дуги к следующей фонеме или к финальному состоянию\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     output_label \u001b[38;5;241m=\u001b[39m word_id \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(tran_ids) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# <eps> для всех, кроме последней фонемы\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WFST' object has no attribute 'add_state'"
     ]
    }
   ],
   "source": [
    "h_fst = create_h_wfst()\n",
    "l_fst = create_Lwfst_from_file(stay_in_silence_penalty=0.0, word_insertion_penalty=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3549d2da-0501-4be5-be8b-ec6ea8085f01",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l_fst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# пробуем разные варианты\u001b[39;00m\n\u001b[1;32m      2\u001b[0m ASR \u001b[38;5;241m=\u001b[39m BeamSearcher(am2phone_mapping\u001b[38;5;241m=\u001b[39mAM_PHONES, \n\u001b[0;32m----> 3\u001b[0m                    graph\u001b[38;5;241m=\u001b[39mOnTheFlyCompose([h_fst, \u001b[43ml_fst\u001b[49m]),\n\u001b[1;32m      4\u001b[0m                    beam_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)   \n\u001b[1;32m      5\u001b[0m hyp \u001b[38;5;241m=\u001b[39m ASR\u001b[38;5;241m.\u001b[39mdecode(example)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(hyp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWER: \u001b[39m\u001b[38;5;124m\"\u001b[39m, jiwer\u001b[38;5;241m.\u001b[39mwer(example_ref, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(hyp\u001b[38;5;241m.\u001b[39mwords)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'l_fst' is not defined"
     ]
    }
   ],
   "source": [
    "# пробуем разные варианты\n",
    "ASR = BeamSearcher(am2phone_mapping=AM_PHONES, \n",
    "                   graph=OnTheFlyCompose([h_fst, l_fst]),\n",
    "                   beam_size=0)   \n",
    "hyp = ASR.decode(example)\n",
    "print(hyp, \"WER: \", jiwer.wer(example_ref, ' '.join(hyp.words)))\n",
    "\n",
    "#Hypothesis(score=inf, time=396, words=[], state=(0, 28544)) WER:  1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b8588eab-859f-4b12-9030-e71e0be28573",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l_fst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ASR \u001b[38;5;241m=\u001b[39m BeamSearcher(am2phone_mapping\u001b[38;5;241m=\u001b[39mAM_PHONES, \n\u001b[0;32m----> 2\u001b[0m                    graph\u001b[38;5;241m=\u001b[39mOnTheFlyCompose([h_fst, \u001b[43ml_fst\u001b[49m]),\n\u001b[1;32m      3\u001b[0m                    beam_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m      4\u001b[0m hyp \u001b[38;5;241m=\u001b[39m ASR\u001b[38;5;241m.\u001b[39mdecode(example)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(hyp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWER: \u001b[39m\u001b[38;5;124m\"\u001b[39m, jiwer\u001b[38;5;241m.\u001b[39mwer(example_ref, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(hyp\u001b[38;5;241m.\u001b[39mwords)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'l_fst' is not defined"
     ]
    }
   ],
   "source": [
    "ASR = BeamSearcher(am2phone_mapping=AM_PHONES, \n",
    "                   graph=OnTheFlyCompose([h_fst, l_fst]),\n",
    "                   beam_size=6)\n",
    "hyp = ASR.decode(example)\n",
    "print(hyp, \"WER: \", jiwer.wer(example_ref, ' '.join(hyp.words)))\n",
    "\n",
    "#Hypothesis(score=-1260.1740928390343, \n",
    "# time=396, \n",
    "# words=['she', 'had', 'ya', 'earn', 'dark', 'soothe', 'doing', 'greasy', 'watch', 'show', 'watch', 'her', 'all', 'year', 'earn'], \n",
    "# state=(0, 0)) WER:  0.9090909090909091"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3c37279e-6610-44e7-b43f-ea831ef52d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /mnt/c/Users/User/Git/asr_itmo_practice/resources/lab4/3gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "The ARPA file is missing <unk>.  Substituting log10 probability -100.\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'l_fst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m lm \u001b[38;5;241m=\u001b[39m kenlmLikeWFST(kenlm\u001b[38;5;241m.\u001b[39mLanguageModel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresources/lab4/3gram.arpa\u001b[39m\u001b[38;5;124m'\u001b[39m), lmwt_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m      2\u001b[0m ASR \u001b[38;5;241m=\u001b[39m BeamSearcher(am2phone_mapping\u001b[38;5;241m=\u001b[39mAM_PHONES, \n\u001b[0;32m----> 3\u001b[0m                    graph\u001b[38;5;241m=\u001b[39mOnTheFlyCompose([h_fst, \u001b[43ml_fst\u001b[49m, lm]),\n\u001b[1;32m      4\u001b[0m                    beam_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m      5\u001b[0m hyp \u001b[38;5;241m=\u001b[39m ASR\u001b[38;5;241m.\u001b[39mdecode(example)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(hyp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWER: \u001b[39m\u001b[38;5;124m\"\u001b[39m, jiwer\u001b[38;5;241m.\u001b[39mwer(example_ref, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(hyp\u001b[38;5;241m.\u001b[39mwords)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'l_fst' is not defined"
     ]
    }
   ],
   "source": [
    "lm = kenlmLikeWFST(kenlm.LanguageModel('resources/lab4/3gram.arpa'), lmwt_factor=0.2)\n",
    "ASR = BeamSearcher(am2phone_mapping=AM_PHONES, \n",
    "                   graph=OnTheFlyCompose([h_fst, l_fst, lm]),\n",
    "                   beam_size=6)\n",
    "hyp = ASR.decode(example)\n",
    "print(hyp, \"WER: \", jiwer.wer(example_ref, ' '.join(hyp.words)))\n",
    "\n",
    "# Hypothesis(score=-1207.7367231749959, \n",
    "# time=396, \n",
    "# words=['she', 'had', 'your', 'earn', 'dark', 'soothe', 'doing', 'greasy', 'watch', 'she', 'water', 'all', 'year', 'earn'], \n",
    "# state=(0, 0, <kenlm.State object at 0x7f9ad849fcf0>)) WER:  0.6363636363636364"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81474c81-99af-497a-a1f1-e9b85e8c39d5",
   "metadata": {},
   "source": [
    "### финальное задание\n",
    "Постройте графики зависимости WER на example и времени работы декодирования от таких параметров как: \n",
    "* lmwt_factor\n",
    "* stay_in_silence_penalty\n",
    "* word_insertion_penalty\n",
    "* beam\n",
    "\n",
    "Выберите оптимальные по соотношению WER/time параметры. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d20e693-bcb3-4937-907a-9c1546795a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654550e1-7c49-4008-94f5-60ae6d3b334c",
   "metadata": {},
   "source": [
    "# Дополнительное задание (2 балла)\n",
    "Декодируйте всю коллекцию am_nlogprobs с подобраными ранее параметрами и посчитайте WER на этой выборке\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a85ca6-a620-4828-abc7-721df929452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
